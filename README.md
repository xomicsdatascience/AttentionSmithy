# AttentionSmithy

The [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) paper completely revolutionized the AI industry. After inspiring such programs like GPT and BERT, it seems all deep learning research began exclusively focusing on the attention mechanism behind transformers. This has created a great deal of research surrounding the topic, spawning hundreds of variations to the original paper meant to enhance the original program or tailor it to new applications. Most of these developments happen in isolation, disconnected from the broader community and incompatible with tools made by other developers. For developers that want to experiment with combining these ideas to fit a new problem, such a disjointed state is frustrating.

AttentionSmithy was designed as a platform that allows for flexible experimentation with the attention mechanism in a variety of applications. This includes the ability to use a multitude of positional embeddings, variations on the attention mechanism, and others.

The baseline code was originally inspired by [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) blog code.

## AttentionSmithy Components

Here is a visual depiction of how each component fits into the original whole, using Figure 1 from Attention Is All You Need.

![Presentation1](https://github.com/user-attachments/assets/086a7a10-9dc5-4a68-b966-b169c5dbc2c0)

## AttentionSmithy Numeric Embedding

Here is a visual depiction of where each positional or numeric embedding fits in to the original model.

![Presentation2](https://github.com/user-attachments/assets/ccd45af4-8cd7-4922-b577-2c6b32d1a8b6)

## AttentionSmithy Attention Methods

Here is a basic visual of possible attention mechanisms offered in AttentionSmithy. This includes Big Bird attention, from [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062).

![Presentation3](https://github.com/user-attachments/assets/a7edb99f-6e1f-4370-9723-cf33dfa54dae)


